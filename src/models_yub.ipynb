{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detection of modified images or videos using Neural Networks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import io\n",
    "import shutil\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "import pywt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, Activation, Input, GlobalAveragePooling2D\n",
    "from keras.applications import ResNet50, MobileNet, MobileNetV2, VGG16, Xception, EfficientNetB1, EfficientNetB2, EfficientNetB3, EfficientNetV2B1\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "K.clear_session()\n",
    "tf.compat.v1.reset_default_graph()\n",
    "tf.compat.v1.enable_eager_execution()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"sophatvathana/casia-dataset\"\n",
    "PATH_DATASET = './../dataset/'\n",
    "\n",
    "def download_dataset():\n",
    "    api = KaggleApi()\n",
    "    api.authenticate()\n",
    "    print(\"Downloading files...\")\n",
    "    api.dataset_download_files('sophatvathana/casia-dataset', path=PATH_DATASET, unzip=True)\n",
    "\n",
    "    print(\"\\rDownload complete.\")\n",
    "\n",
    "\n",
    "def clean_directory():\n",
    "    print(\"Moving folder...\")\n",
    "    os.rename(PATH_DATASET+\"CASIA2/Au\", PATH_DATASET+\"Au\")\n",
    "    os.rename(PATH_DATASET+\"CASIA2/Tp\", PATH_DATASET+\"Tp\")\n",
    "    \n",
    "    print(\"Cleaning directory...\")\n",
    "    shutil.rmtree(PATH_DATASET+\"casia\")\n",
    "    shutil.rmtree(PATH_DATASET+\"CASIA1\")\n",
    "    shutil.rmtree(PATH_DATASET+\"CASIA2\")\n",
    "    os.remove(PATH_DATASET+\"Tp/Thumbs.db\")\n",
    "    os.remove(PATH_DATASET+\"Au/Thumbs.db\")\n",
    "    print(\"Cleaning complete.\")\n",
    "\n",
    "def remove_images():\n",
    "    print(\"Removing images...\")\n",
    "    for i in range(1, 4):\n",
    "        os.remove(PATH_DATASET+\"Au/Au (\"+str(i)+\").jpg\")\n",
    "        os.remove(PATH_DATASET+\"Tp/Tp (\"+str(i)+\").jpg\")\n",
    "    print(\"Removing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(PATH_DATASET+\"Au\"):\n",
    "    download_dataset()\n",
    "    clean_directory()\n",
    "else:\n",
    "    print(\"Dataset already Downloaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REAL_IMAGE_PATH = '../dataset/Au'\n",
    "FAKE_IMAGE_PATH = \"../dataset/Tp\"\n",
    "IMG_SIZE = (256, 256)\n",
    "CLASS = [\"real\", \"fake\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cabezera_au = \"category\", \"image\", \"class\"\n",
    "df_au = pd.DataFrame(columns=cabezera_au)\n",
    "\n",
    "cabezera_tp = \"category\", \"image\", \"region\", \"class\"\n",
    "df_tp = pd.DataFrame(columns=cabezera_tp)\n",
    "\n",
    "for idx, file in enumerate(os.listdir(REAL_IMAGE_PATH)):\n",
    "    img_path = os.path.join(REAL_IMAGE_PATH, file)\n",
    "    category = file.split(\"_\")\n",
    "\n",
    "    df_au = pd.concat([df_au, pd.DataFrame([[category[1], img_path, CLASS[0]]], columns=cabezera_au)], ignore_index=True)\n",
    "\n",
    "df_au = df_au[df_au.category != \"txt\"]\n",
    "df_au = df_au[df_au.category != \"ind\"]\n",
    "df_au = df_au.groupby('category').head(600)\n",
    "\n",
    "for file in os.listdir(FAKE_IMAGE_PATH):\n",
    "    #convert image to np array\n",
    "    img_path = os.path.join(FAKE_IMAGE_PATH, file)\n",
    "\n",
    "    category = file.split(\"_\")\n",
    "    category[5] = category[5][:3]\n",
    "    df_tp = pd.concat([df_tp, pd.DataFrame([[category[5], img_path, category[1], CLASS[1]]], columns=cabezera_tp)], ignore_index=True)\n",
    "\n",
    "df_tp = df_tp[df_tp.category != \"txt\"]\n",
    "df_tp = df_tp[df_tp.category != \"ind\"]\n",
    "df_tp = df_tp.groupby(['category', 'region']).head(300)\n",
    "df_tp = df_tp.drop(columns=['region'])\n",
    "\n",
    "df = pd.concat([df_au, df_tp], ignore_index=True)\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Preprocessing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yuv(imagen):\n",
    "    imagen_yuv = cv2.cvtColor(imagen, cv2.COLOR_RGB2YUV)\n",
    "    \n",
    "    canal_u = imagen_yuv[:,:,1]\n",
    "    \n",
    "    return canal_u\n",
    "\n",
    "def preprocess_image(image):\n",
    "    image = yuv(image)\n",
    "    image = np.expand_dims(image, axis=-1)\n",
    "    image = image / 255.0\n",
    "\n",
    "    return image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating and splitting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_image,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_image,\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    dataframe=df_train,\n",
    "    directory=None,\n",
    "    x_col=\"image\",\n",
    "    y_col=\"class\",\n",
    "    subset=\"training\",\n",
    "    batch_size=batch_size,\n",
    "    seed=42,\n",
    "    shuffle=True,\n",
    "    class_mode=\"binary\",\n",
    "    target_size=IMG_SIZE\n",
    ")\n",
    "\n",
    "val_generator = train_datagen.flow_from_dataframe(\n",
    "    dataframe=df_train,\n",
    "    directory=None,\n",
    "    x_col=\"image\",\n",
    "    y_col=\"class\",\n",
    "    subset=\"validation\",\n",
    "    batch_size=batch_size,\n",
    "    seed=42,\n",
    "    shuffle=True,\n",
    "    class_mode=\"binary\",\n",
    "    target_size=IMG_SIZE\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_dataframe(\n",
    "    dataframe=df_test,\n",
    "    directory=None,\n",
    "    x_col=\"image\",\n",
    "    y_col=\"class\",\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    class_mode=\"binary\",\n",
    "    target_size=IMG_SIZE\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    tf.keras.metrics.BinaryAccuracy(name='binary_accuracy'),\n",
    "    tf.keras.metrics.Precision(name='precision'),\n",
    "    tf.keras.metrics.Recall(name='recall'),\n",
    "    tf.keras.metrics.AUC(name='auc'),\n",
    "    tf.keras.metrics.AUC(name='prc', curve='PR')\n",
    "]\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    min_delta=0, \n",
    "    patience=10, \n",
    "    verbose=0, \n",
    "    mode='auto', \n",
    "    baseline=None, \n",
    "    restore_best_weights=False\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1)\n",
    "\n",
    "\n",
    "model_chekpoint = ModelCheckpoint(\n",
    "    filepath='./../model/checkpoints', \n",
    "    monitor='val_loss', \n",
    "    verbose=0, \n",
    "    save_best_only=True,\n",
    "    save_weights_only=True, \n",
    "    mode='auto', \n",
    "    save_freq='epoch'\n",
    ")\n",
    "\n",
    "tensor_board = TensorBoard(\n",
    "    log_dir='./../model/logs',\n",
    "    histogram_freq=0,\n",
    "    write_graph=True,\n",
    "    write_images=False,\n",
    "    update_freq='epoch',\n",
    "    profile_batch=2,\n",
    "    embeddings_freq=0,\n",
    "    embeddings_metadata=None\n",
    ")\n",
    "\n",
    "callbacks = [early_stopping, model_chekpoint, tensor_board]\n",
    "optimizer = Adam(1e-3)\n",
    "loss = keras.losses.BinaryCrossentropy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the model using transfer learning (MobileNet)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "detect_manipulated_images_model_mobilenet.h5 14m 27s 19 epocas bacth_size=32 45s\n",
    "\n",
    "loss: 0.1897 - binary_accuracy: 0.9390 - precision: 0.9819 - recall: 0.9148 - auc: 0.9813 - prc: 0.9889"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MobileNet(weights='imagenet', include_top=False, input_shape=(256, 256, 3))\n",
    "\n",
    "x = GlobalAveragePooling2D()(model.output)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=model.inputs, outputs=predictions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the model using transfer learning (VGG16)\n",
    "\n",
    "detect_manipulated_images_model_vgg16_v1.h5 36m 15s 14 epocas bacth_size=32 89s\n",
    "\n",
    "loss: 0.6463 - binary_accuracy: 0.6346 - precision: 0.6471 - recall: 0.9031 - auc: 0.6137 - prc: 0.6960\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar la arquitectura pre-entrenada VGG-16 sin las capas completamente conectadas\n",
    "vgg16 = VGG16(weights='imagenet', include_top=False, input_shape=(256, 256, 3))\n",
    "\n",
    "# Definir una nueva capa de salida personalizada\n",
    "x = vgg16.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# Construir el modelo final que incluye VGG-16 y la nueva capa de salida\n",
    "model = Model(inputs=vgg16.inputs, outputs=predictions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creatin Model Xception\n",
    "\n",
    "detect_manipulated_images_model_Xception.h5 43m 34s 20 epocas bacth_size=32 130s\n",
    "\n",
    "\n",
    "loss: 0.8319 - binary_accuracy: 0.8171 - precision: 0.7771 - recall: 0.9820 - auc: 0.9086 - prc: 0.9015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(256, 256, 3))\n",
    "model = Xception(weights='imagenet', include_top=False, input_shape=(256, 256, 3))\n",
    "\n",
    "x = GlobalAveragePooling2D()(model.output)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=model.inputs, outputs=predictions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creatin Model EfficientNetB1\n",
    "\n",
    "detect_manipulated_images_model_EfficientNetB1.h5 32m 36s 18 epocas bacth_size=32 104s\n",
    "\n",
    "loss: 0.6903 - binary_accuracy: 0.6254 - precision: 0.6249 - recall: 0.9958 - auc: 0.4850 - prc: 0.5805"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EfficientNetB1(weights='imagenet', include_top=False, input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3))\n",
    "\n",
    "x = GlobalAveragePooling2D()(model.output)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=model.inputs, outputs=predictions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "history = model.fit(train_generator, epochs=50, batch_size=batch_size, validation_data=val_generator, callbacks=callbacks)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_generator)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(history):\n",
    "    metrics = ['binary_accuracy', 'loss', 'prc', 'precision', 'recall']\n",
    "    fig, axes = plt.subplots(len(metrics), 1, figsize=(10, 10))\n",
    "    \n",
    "    for i, metric in enumerate(metrics):\n",
    "        axes[i].plot(history.history[metric], label='train')\n",
    "        axes[i].plot(history.history[f'val_{metric}'], label='val')\n",
    "        axes[i].set_title(metric)\n",
    "        axes[i].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(model, X, y_true):\n",
    "    y_pred = model.predict(X) > 0.5\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.imshow(cm, cmap=plt.cm.Reds)\n",
    "    plt.title('Confusion Matrix', fontsize=16)\n",
    "    plt.ylabel('True label', fontsize=14)\n",
    "    plt.xlabel('Predicted label', fontsize=14)\n",
    "    plt.xticks([0, 1], ['Manipulated', 'Original'], fontsize=12)\n",
    "    plt.yticks([0, 1], ['Manipulated', 'Original'], fontsize=12)\n",
    "    plt.colorbar()\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            plt.text(j, i, str(cm[i][j]), ha='center', va='center', fontsize=20)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./../model/yuv_models/detect_manipulated_images_model_mobilenet.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e8a00227fbbeabfee3e4c7eae78ea7efab3aaaa5b33f3ff28071daefabaeab66"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

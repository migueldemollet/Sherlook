{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detection of modified images or videos using Neural Networks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageChops, ImageFilter\n",
    "import pandas as pd\n",
    "import os\n",
    "import io\n",
    "import shutil\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import backend as K\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, Activation, Input, Lambda, Resizing, Rescaling, GlobalAveragePooling2D, RandomFlip, RandomRotation, RandomZoom, RandomCrop, RandomTranslation, RandomContrast\n",
    "from keras.applications import ResNet50, MobileNet, MobileNetV2, VGG16, Xception\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
    "import matplotlib.cm as cm\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"sophatvathana/casia-dataset\"\n",
    "PATH_DATASET = './../dataset/'\n",
    "\n",
    "def download_dataset():\n",
    "    api = KaggleApi()\n",
    "    api.authenticate()\n",
    "    print(\"Downloading files...\")\n",
    "    api.dataset_download_files('sophatvathana/casia-dataset', path=PATH_DATASET, unzip=True)\n",
    "\n",
    "    print(\"\\rDownload complete.\")\n",
    "\n",
    "\n",
    "def clean_directory():\n",
    "    print(\"Moving folder...\")\n",
    "    os.rename(PATH_DATASET+\"CASIA2/Au\", PATH_DATASET+\"Au\")\n",
    "    os.rename(PATH_DATASET+\"CASIA2/Tp\", PATH_DATASET+\"Tp\")\n",
    "    \n",
    "    print(\"Cleaning directory...\")\n",
    "    shutil.rmtree(PATH_DATASET+\"casia\")\n",
    "    shutil.rmtree(PATH_DATASET+\"CASIA1\")\n",
    "    shutil.rmtree(PATH_DATASET+\"CASIA2\")\n",
    "    os.remove(PATH_DATASET+\"Tp/Thumbs.db\")\n",
    "    os.remove(PATH_DATASET+\"Au/Thumbs.db\")\n",
    "    print(\"Cleaning complete.\")\n",
    "\n",
    "def remove_images():\n",
    "    print(\"Removing images...\")\n",
    "    for i in range(1, 4):\n",
    "        os.remove(PATH_DATASET+\"Au/Au (\"+str(i)+\").jpg\")\n",
    "        os.remove(PATH_DATASET+\"Tp/Tp (\"+str(i)+\").jpg\")\n",
    "    print(\"Removing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already Downloaded.\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(PATH_DATASET+\"Au\"):\n",
    "    download_dataset()\n",
    "    clean_directory()\n",
    "else:\n",
    "    print(\"Dataset already Downloaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "REAL_IMAGE_PATH = '../dataset/Au'\n",
    "FAKE_IMAGE_PATH = \"../dataset/Tp\"\n",
    "IMG_SIZE = (256, 256)\n",
    "CLASS = [0.0, 1.0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>image</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sec</td>\n",
       "      <td>../dataset/Tp\\Tp_S_CRN_S_N_sec00033_sec00033_0...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>art</td>\n",
       "      <td>../dataset/Tp\\Tp_S_NNN_S_N_art00059_art00059_0...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cha</td>\n",
       "      <td>../dataset/Tp\\Tp_D_CRN_S_N_cha00028_art00013_0...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ani</td>\n",
       "      <td>../dataset/Au\\Au_ani_10188.jpg</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pla</td>\n",
       "      <td>../dataset/Au\\Au_pla_30221.jpg</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  category                                              image  class\n",
       "0      sec  ../dataset/Tp\\Tp_S_CRN_S_N_sec00033_sec00033_0...    1.0\n",
       "1      art  ../dataset/Tp\\Tp_S_NNN_S_N_art00059_art00059_0...    1.0\n",
       "2      cha  ../dataset/Tp\\Tp_D_CRN_S_N_cha00028_art00013_0...    1.0\n",
       "3      ani                     ../dataset/Au\\Au_ani_10188.jpg    0.0\n",
       "4      pla                     ../dataset/Au\\Au_pla_30221.jpg    0.0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cabezera_au = \"category\", \"image\", \"class\"\n",
    "df_au = pd.DataFrame(columns=cabezera_au)\n",
    "\n",
    "cabezera_tp = \"category\", \"image\", \"region\", \"class\"\n",
    "df_tp = pd.DataFrame(columns=cabezera_tp)\n",
    "\n",
    "for idx, file in enumerate(os.listdir(REAL_IMAGE_PATH)):\n",
    "    img_path = os.path.join(REAL_IMAGE_PATH, file)\n",
    "    category = file.split(\"_\")\n",
    "\n",
    "    df_au = pd.concat([df_au, pd.DataFrame([[category[1], img_path, CLASS[0]]], columns=cabezera_au)], ignore_index=True)\n",
    "\n",
    "df_au = df_au[df_au.category != \"txt\"]\n",
    "df_au = df_au[df_au.category != \"ind\"]\n",
    "df_au = df_au.groupby('category').head(600)\n",
    "\n",
    "for file in os.listdir(FAKE_IMAGE_PATH):\n",
    "    #convert image to np array\n",
    "    img_path = os.path.join(FAKE_IMAGE_PATH, file)\n",
    "\n",
    "    category = file.split(\"_\")\n",
    "    category[5] = category[5][:3]\n",
    "    df_tp = pd.concat([df_tp, pd.DataFrame([[category[5], img_path, category[1], CLASS[1]]], columns=cabezera_tp)], ignore_index=True)\n",
    "\n",
    "df_tp = df_tp[df_tp.category != \"txt\"]\n",
    "df_tp = df_tp[df_tp.category != \"ind\"]\n",
    "df_tp = df_tp.groupby(['category', 'region']).head(300)\n",
    "df_tp = df_tp.drop(columns=['region'])\n",
    "\n",
    "df = pd.concat([df_au, df_tp], ignore_index=True)\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Preprocessing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ela(image, quality=99):\n",
    "    # Comprimir y descomprimir la imagen\n",
    "    _, buffer = cv2.imencode('.jpg', image, [cv2.IMWRITE_JPEG_QUALITY, quality])\n",
    "    compressed_image = cv2.imdecode(np.frombuffer(buffer, np.uint8), cv2.IMREAD_COLOR)\n",
    "\n",
    "    #convert compressed_image uint8 to float32\n",
    "    #compressed_image = compressed_image.astype(np.float32)\n",
    "\n",
    "    diff = 15 * cv2.absdiff(image, compressed_image)\n",
    "    \n",
    "    return diff\n",
    "\n",
    "def ela_v2(orig_img, quality=99):\n",
    "    _, buffer = cv2.imencode('.jpg', orig_img, [cv2.IMWRITE_JPEG_QUALITY, quality])\n",
    "    compressed_img = cv2.imdecode(np.frombuffer(buffer, np.uint8), cv2.IMREAD_COLOR)\n",
    "\n",
    "    orig_img = orig_img.astype(np.uint8)\n",
    "\n",
    "    diff = 15 * cv2.absdiff(orig_img, compressed_img)\n",
    "    gray_diff = cv2.cvtColor(diff, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Aplicar umbralización para convertir en una imagen binaria\n",
    "    threshold_value, binary_image = cv2.threshold(gray_diff, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "    # Multiplicar la imagen binaria por la imagen de diferencia original para resaltar los píxeles falsos\n",
    "    diff_highlighted = cv2.bitwise_and(diff, diff, mask=binary_image)\n",
    "\n",
    "    # Aplicar filtrado Gaussiano para reducir el ruido\n",
    "    diff_filtered = cv2.GaussianBlur(diff_highlighted, (5, 5), 0)\n",
    "\n",
    "    return diff_filtered\n",
    "\n",
    "from keras.applications.mobilenet import preprocess_input\n",
    "\n",
    "def preprocess_image(img_path):\n",
    "    img = cv2.imread(img_path.numpy().decode())\n",
    "    img = cv2.resize(img, IMG_SIZE, interpolation=cv2.INTER_AREA)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return preprocess_input(ela(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augmentation(image, label):\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_flip_up_down(image)\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((df['image'].values, df['class'].values))\n",
    "#dataset = dataset.map(lambda x, y: (tf.py_function(preprocess_image, [x], tf.float32), y))\n",
    "dataset = dataset.map(lambda x, y: (tf.py_function(preprocess_image, [x], tf.float32), tf.cast(y, tf.float32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "dataset.shuffle(len(df))\n",
    "train = int(0.6 * len(df))\n",
    "val = int(0.2 * len(df))\n",
    "test = int(0.2 * len(df))\n",
    "\n",
    "train_dataset = dataset.skip(val+test).batch(BATCH_SIZE).map(data_augmentation, num_parallel_calls=tf.data.experimental.AUTOTUNE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "val_dataset = dataset.skip(test).take(val).batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "test_dataset = dataset.take(test).batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sample of train: 9368\n",
      "Total sample of validation: 1561\n",
      "Total sample of test: 1561\n"
     ]
    }
   ],
   "source": [
    "# Imprimir el número total de elementos en cada conjunto de datos\n",
    "print(\"Total sample of train:\", train*2)\n",
    "print(\"Total sample of validation:\", val)\n",
    "print(\"Total sample of test:\", test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(CustomModel, self).__init__()\n",
    "\n",
    "        self.base_model = MobileNet(weights='imagenet', include_top=False, input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3))\n",
    "        self.global_pooling = GlobalAveragePooling2D()(self.base_model.output)\n",
    "        self.dense1 = Dense(1024, activation='relu')(self.global_pooling)\n",
    "        self.output_layer = Dense(1, activation='sigmoid')(self.dense1)\n",
    "\n",
    "        self.model = Model(inputs=self.base_model.inputs, outputs=self.output_layer)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.model(inputs)\n",
    "    \n",
    "    def predict(self, image, grad_cam=False):\n",
    "        img_preprocessed = self._preprocess_image(image)\n",
    "        if grad_cam:\n",
    "            return self._predict_with_grad_cam(image, img_preprocessed)\n",
    "        else:\n",
    "            return super(CustomModel, self).predict(img_preprocessed)\n",
    "        \n",
    "    def _preprocess_image(self, img):\n",
    "        img = cv2.resize(img, IMG_SIZE, interpolation=cv2.INTER_AREA)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        ela_image = ela(img)\n",
    "        img_expanded = np.expand_dims(ela_image, axis=0)  \n",
    "        return preprocess_input(img_expanded)\n",
    "\n",
    "    def _get_last_conv_layer(self):\n",
    "        for layer in reversed(self.model.layers):\n",
    "            if isinstance(layer, tf.keras.layers.Conv2D):\n",
    "                return layer.name\n",
    "        return None\n",
    "    \n",
    "    def _predict_with_grad_cam(self, image, preprocess_image):\n",
    "        image = cv2.resize(image, IMG_SIZE, interpolation=cv2.INTER_AREA)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        grad_model = tf.keras.models.Model(\n",
    "            [self.model.inputs], [self.model.get_layer(self._get_last_conv_layer()).output, self.model.output]\n",
    "        )\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            last_conv_layer_output, preds = grad_model(preprocess_image)\n",
    "            pred_index = tf.argmax(preds[0])\n",
    "            class_channel = preds[:, pred_index]\n",
    "\n",
    "        grads = tape.gradient(class_channel, last_conv_layer_output)\n",
    "        pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "        last_conv_layer_output = last_conv_layer_output[0]\n",
    "        heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n",
    "        heatmap = tf.squeeze(heatmap)\n",
    "        heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
    "        heatmap = heatmap.numpy()\n",
    "\n",
    "        heatmap = np.uint8(255 * heatmap)\n",
    "\n",
    "        # Use jet colormap to colorize heatmap\n",
    "        jet = cm.get_cmap(\"jet\")\n",
    "\n",
    "        # Use RGB values of the colormap\n",
    "        jet_colors = jet(np.arange(256))[:, :3]\n",
    "        jet_heatmap = jet_colors[heatmap]\n",
    "\n",
    "        # Create an image with RGB colorized heatmap\n",
    "        jet_heatmap = keras.utils.array_to_img(jet_heatmap)\n",
    "        jet_heatmap = jet_heatmap.resize((image.shape[1], image.shape[0]))\n",
    "        jet_heatmap = keras.utils.img_to_array(jet_heatmap)\n",
    "\n",
    "        # Superimpose the heatmap on the original image\n",
    "        superimposed_img = jet_heatmap * 0.4 + image\n",
    "        superimposed_img = keras.utils.array_to_img(superimposed_img)\n",
    "\n",
    "        return class_channel[0].numpy(), np.array(superimposed_img)\n",
    "\n",
    "    def save_model(self, filepath):\n",
    "    # Create the directory if it doesn't exist\n",
    "        os.makedirs(filepath, exist_ok=True)\n",
    "\n",
    "        # Save the model architecture and weights\n",
    "        self.model.save_weights(os.path.join(filepath, 'model_weights.h5'))\n",
    "\n",
    "        model_json = self.model.to_json()\n",
    "        with open(os.path.join(filepath, 'model_architecture.json'), 'w') as json_file:\n",
    "            json_file.write(model_json)\n",
    "    \n",
    "    @classmethod\n",
    "    def load_model(cls, filepath):\n",
    "        # Load the model architecture and weights\n",
    "        with open(filepath + '/model_architecture.json', 'r') as json_file:\n",
    "            model_json = json_file.read()\n",
    "        loaded_model = cls._from_json(model_json)\n",
    "\n",
    "        loaded_model.model.load_weights(filepath + '/model_weights.h5')\n",
    "        return loaded_model\n",
    "\n",
    "    @classmethod\n",
    "    def _from_json(cls, model_json):\n",
    "        # Create a model instance from JSON architecture\n",
    "        model = cls()\n",
    "        model.model = tf.keras.models.model_from_json(model_json)\n",
    "        return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    tf.keras.metrics.BinaryAccuracy(name='binary_accuracy'),\n",
    "    tf.keras.metrics.Precision(name='precision'),\n",
    "    tf.keras.metrics.Recall(name='recall'),\n",
    "    tf.keras.metrics.AUC(name='auc'),\n",
    "    tf.keras.metrics.AUC(name='prc', curve='PR')\n",
    "]\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    min_delta=0, \n",
    "    patience=10, \n",
    "    verbose=0, \n",
    "    mode='auto',\n",
    "    baseline=None, \n",
    "    restore_best_weights=False\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001)\n",
    "\n",
    "\n",
    "model_chekpoint = ModelCheckpoint(\n",
    "    filepath='./../model/checkpoints', \n",
    "    monitor='val_loss', \n",
    "    verbose=0, \n",
    "    save_best_only=True,\n",
    "    save_weights_only=True, \n",
    "    mode='auto', \n",
    "    save_freq='epoch'\n",
    ")\n",
    "\n",
    "tensor_board = TensorBoard(\n",
    "    log_dir='./../model/logs',\n",
    "    histogram_freq=0,\n",
    "    write_graph=True,\n",
    "    write_images=False,\n",
    "    update_freq='epoch',\n",
    "    profile_batch=2,\n",
    "    embeddings_freq=0,\n",
    "    embeddings_metadata=None\n",
    ")\n",
    "\n",
    "callbacks = [early_stopping, tensor_board]\n",
    "optimizer = Adam(learning_rate=1e-3)\n",
    "loss = keras.losses.BinaryCrossentropy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_epochs = 20\n",
    "fine_tune_epochs = 30\n",
    "total_epochs =  initial_epochs + fine_tune_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "Epoch 1/20\n",
      "147/147 [==============================] - 75s 383ms/step - loss: 0.3501 - binary_accuracy: 0.8406 - precision: 0.8082 - recall: 0.8559 - auc: 0.9281 - prc: 0.9115 - val_loss: 1.9850 - val_binary_accuracy: 0.5804 - val_precision: 0.5278 - val_recall: 1.0000 - val_auc: 0.7814 - val_prc: 0.6920\n",
      "Epoch 2/20\n",
      "147/147 [==============================] - 71s 378ms/step - loss: 0.2495 - binary_accuracy: 0.8715 - precision: 0.8298 - recall: 0.9061 - auc: 0.9569 - prc: 0.9495 - val_loss: 1.1126 - val_binary_accuracy: 0.7572 - val_precision: 0.6630 - val_recall: 0.9809 - val_auc: 0.8704 - val_prc: 0.7933\n",
      "Epoch 3/20\n",
      "147/147 [==============================] - 71s 378ms/step - loss: 0.2273 - binary_accuracy: 0.8792 - precision: 0.8366 - recall: 0.9159 - auc: 0.9632 - prc: 0.9579 - val_loss: 2.4554 - val_binary_accuracy: 0.5484 - val_precision: 0.5094 - val_recall: 0.9986 - val_auc: 0.6662 - val_prc: 0.5889\n",
      "Epoch 4/20\n",
      "147/147 [==============================] - 71s 379ms/step - loss: 0.2241 - binary_accuracy: 0.8860 - precision: 0.8466 - recall: 0.9182 - auc: 0.9649 - prc: 0.9592 - val_loss: 0.4469 - val_binary_accuracy: 0.8366 - val_precision: 0.7601 - val_recall: 0.9522 - val_auc: 0.9320 - val_prc: 0.9167\n",
      "Epoch 5/20\n",
      "147/147 [==============================] - 70s 376ms/step - loss: 0.2144 - binary_accuracy: 0.8905 - precision: 0.8522 - recall: 0.9215 - auc: 0.9683 - prc: 0.9637 - val_loss: 0.3171 - val_binary_accuracy: 0.8680 - val_precision: 0.9046 - val_recall: 0.8033 - val_auc: 0.9523 - val_prc: 0.9527\n",
      "Epoch 6/20\n",
      "147/147 [==============================] - 70s 376ms/step - loss: 0.2045 - binary_accuracy: 0.8974 - precision: 0.8533 - recall: 0.9377 - auc: 0.9710 - prc: 0.9659 - val_loss: 1.2783 - val_binary_accuracy: 0.6413 - val_precision: 0.5703 - val_recall: 0.9536 - val_auc: 0.8510 - val_prc: 0.8124\n",
      "Epoch 7/20\n",
      "147/147 [==============================] - 70s 376ms/step - loss: 0.1969 - binary_accuracy: 0.8999 - precision: 0.8638 - recall: 0.9284 - auc: 0.9745 - prc: 0.9701 - val_loss: 0.5011 - val_binary_accuracy: 0.7649 - val_precision: 0.6895 - val_recall: 0.9071 - val_auc: 0.9124 - val_prc: 0.9112\n",
      "Epoch 8/20\n",
      "147/147 [==============================] - 70s 376ms/step - loss: 0.1821 - binary_accuracy: 0.9089 - precision: 0.8781 - recall: 0.9308 - auc: 0.9777 - prc: 0.9741 - val_loss: 1.2787 - val_binary_accuracy: 0.5926 - val_precision: 0.5388 - val_recall: 0.9098 - val_auc: 0.7865 - val_prc: 0.7654\n",
      "Epoch 9/20\n",
      "147/147 [==============================] - 71s 380ms/step - loss: 0.1687 - binary_accuracy: 0.9157 - precision: 0.8838 - recall: 0.9401 - auc: 0.9808 - prc: 0.9779 - val_loss: 0.3986 - val_binary_accuracy: 0.8168 - val_precision: 0.9407 - val_recall: 0.6503 - val_auc: 0.9345 - val_prc: 0.9327\n",
      "Epoch 10/20\n",
      "147/147 [==============================] - 70s 372ms/step - loss: 0.1800 - binary_accuracy: 0.9119 - precision: 0.8769 - recall: 0.9401 - auc: 0.9787 - prc: 0.9747 - val_loss: 1.6060 - val_binary_accuracy: 0.6079 - val_precision: 0.5482 - val_recall: 0.9331 - val_auc: 0.7787 - val_prc: 0.7150\n",
      "Epoch 11/20\n",
      "147/147 [==============================] - 70s 371ms/step - loss: 0.1570 - binary_accuracy: 0.9253 - precision: 0.8990 - recall: 0.9433 - auc: 0.9833 - prc: 0.9803 - val_loss: 0.2699 - val_binary_accuracy: 0.8860 - val_precision: 0.8403 - val_recall: 0.9344 - val_auc: 0.9586 - val_prc: 0.9522\n",
      "Epoch 12/20\n",
      "147/147 [==============================] - 70s 371ms/step - loss: 0.1677 - binary_accuracy: 0.9193 - precision: 0.8873 - recall: 0.9442 - auc: 0.9810 - prc: 0.9774 - val_loss: 0.5644 - val_binary_accuracy: 0.7848 - val_precision: 0.7050 - val_recall: 0.9303 - val_auc: 0.9167 - val_prc: 0.9113\n",
      "Epoch 13/20\n",
      "147/147 [==============================] - 70s 370ms/step - loss: 0.1509 - binary_accuracy: 0.9292 - precision: 0.9023 - recall: 0.9484 - auc: 0.9845 - prc: 0.9820 - val_loss: 1.6656 - val_binary_accuracy: 0.6464 - val_precision: 0.9945 - val_recall: 0.2473 - val_auc: 0.8164 - val_prc: 0.8492\n",
      "Epoch 14/20\n",
      "147/147 [==============================] - 70s 371ms/step - loss: 0.1482 - binary_accuracy: 0.9253 - precision: 0.8904 - recall: 0.9549 - auc: 0.9854 - prc: 0.9835 - val_loss: 0.3960 - val_binary_accuracy: 0.8373 - val_precision: 0.8944 - val_recall: 0.7404 - val_auc: 0.9485 - val_prc: 0.9398\n",
      "Epoch 15/20\n",
      "147/147 [==============================] - 70s 370ms/step - loss: 0.1489 - binary_accuracy: 0.9281 - precision: 0.9057 - recall: 0.9414 - auc: 0.9856 - prc: 0.9828 - val_loss: 1.0048 - val_binary_accuracy: 0.6963 - val_precision: 0.6079 - val_recall: 0.9932 - val_auc: 0.8810 - val_prc: 0.8401\n",
      "Epoch 16/20\n",
      "147/147 [==============================] - 70s 373ms/step - loss: 0.1347 - binary_accuracy: 0.9324 - precision: 0.9109 - recall: 0.9452 - auc: 0.9882 - prc: 0.9865 - val_loss: 0.5640 - val_binary_accuracy: 0.8463 - val_precision: 0.7808 - val_recall: 0.9344 - val_auc: 0.9215 - val_prc: 0.8751\n",
      "Epoch 17/20\n",
      "147/147 [==============================] - 72s 383ms/step - loss: 0.1338 - binary_accuracy: 0.9362 - precision: 0.9142 - recall: 0.9503 - auc: 0.9883 - prc: 0.9860 - val_loss: 0.4610 - val_binary_accuracy: 0.8616 - val_precision: 0.7860 - val_recall: 0.9686 - val_auc: 0.9456 - val_prc: 0.9163\n",
      "Epoch 18/20\n",
      "147/147 [==============================] - 72s 382ms/step - loss: 0.1384 - binary_accuracy: 0.9351 - precision: 0.9151 - recall: 0.9466 - auc: 0.9877 - prc: 0.9860 - val_loss: 0.4364 - val_binary_accuracy: 0.8571 - val_precision: 0.8921 - val_recall: 0.7910 - val_auc: 0.9495 - val_prc: 0.9382\n",
      "Epoch 19/20\n",
      "147/147 [==============================] - 71s 377ms/step - loss: 0.1265 - binary_accuracy: 0.9413 - precision: 0.9264 - recall: 0.9475 - auc: 0.9895 - prc: 0.9870 - val_loss: 0.3760 - val_binary_accuracy: 0.8994 - val_precision: 0.8758 - val_recall: 0.9153 - val_auc: 0.9541 - val_prc: 0.9293\n",
      "Epoch 20/20\n",
      "147/147 [==============================] - 70s 372ms/step - loss: 0.1211 - binary_accuracy: 0.9452 - precision: 0.9293 - recall: 0.9531 - auc: 0.9908 - prc: 0.9890 - val_loss: 0.3295 - val_binary_accuracy: 0.8744 - val_precision: 0.8602 - val_recall: 0.8743 - val_auc: 0.9577 - val_prc: 0.9402\n"
     ]
    }
   ],
   "source": [
    "model = CustomModel()\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=initial_epochs,\n",
    "    batch_size=BATCH_SIZE, \n",
    "    validation_data=val_dataset,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - 10s 192ms/step - loss: 0.3033 - binary_accuracy: 0.8892 - precision: 0.8611 - recall: 0.9075 - auc: 0.9606 - prc: 0.9484\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.30329713225364685,\n",
       " 0.8891736268997192,\n",
       " 0.861074686050415,\n",
       " 0.9074585437774658,\n",
       " 0.9606494307518005,\n",
       " 0.9484180808067322]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(history):\n",
    "    # Obtener las métricas de entrenamiento\n",
    "    loss = history.history['loss']\n",
    "    accuracy = history.history['binary_accuracy']\n",
    "    \n",
    "    # Obtener las métricas de validación si están disponibles\n",
    "    if 'val_loss' in history.history:\n",
    "        val_loss = history.history['val_loss']\n",
    "        val_accuracy = history.history['val_binary_accuracy']\n",
    "        has_validation = True\n",
    "    else:\n",
    "        has_validation = False\n",
    "    \n",
    "    # Crear los subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8))\n",
    "    \n",
    "    # Plot de la pérdida\n",
    "    ax1.plot(loss, label='Training Loss')\n",
    "    if has_validation:\n",
    "        ax1.plot(val_loss, label='Validation Loss')\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title('Training and Validation Loss')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Plot de la precisión binaria\n",
    "    ax2.plot(accuracy, label='Training Accuracy')\n",
    "    if has_validation:\n",
    "        ax2.plot(val_accuracy, label='Validation Accuracy')\n",
    "    ax2.set_xlabel('Epochs')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.set_title('Training and Validation Accuracy')\n",
    "    ax2.legend()\n",
    "    \n",
    "    # Mostrar el gráfico\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_metrics(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../dataset/Tp/Tp_D_CRD_S_O_ani10103_ani10111_10637.jpg'\n",
    "img = cv2.imread(path)\n",
    "y_pred, img_heamap = model.predict(img, grad_cam=True)\n",
    "\n",
    "img = cv2.resize(img, IMG_SIZE, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "print(y_pred)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "plt.title('Original Image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(ela(img))\n",
    "plt.title('prepocessing Image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(img_heamap)\n",
    "plt.title('Superimposed Image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../dataset/test/me_x_3.jpg'\n",
    "img = cv2.imread(path)\n",
    "\n",
    "y_pred, img_heamap = model.predict(img, grad_cam=True)\n",
    "\n",
    "img = cv2.resize(img, IMG_SIZE, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "print(y_pred)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "plt.title('Original Image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(ela(img))\n",
    "plt.title('prepocessing Image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(img_heamap)\n",
    "plt.title('Superimposed Image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of layers in the base model: \", len(base_model.layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = True\n",
    "\n",
    "fine_tune_at = -10\n",
    "\n",
    "# Freeze all the layers before the `fine_tune_at` layer\n",
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "history_fine = model.fit(\n",
    "    train_generator,\n",
    "    epochs=total_epochs, \n",
    "    initial_epoch=history.epoch[-1],\n",
    "    batch_size=batch_size, \n",
    "    validation_data=val_generator,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['binary_accuracy'] + history_fine.history['binary_accuracy']\n",
    "val_acc = history.history['val_binary_accuracy'] + history_fine.history['val_binary_accuracy']\n",
    "\n",
    "loss = history.history['loss'] + history_fine.history['loss']\n",
    "val_loss = history.history['val_loss'] + history_fine.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.plot([initial_epochs-1,initial_epochs-1],\n",
    "          plt.ylim(), label='Start Fine Tuning')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.ylim([0, 1.0])\n",
    "plt.plot([initial_epochs-1,initial_epochs-1],\n",
    "         plt.ylim(), label='Start Fine Tuning')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_model('../model/custom_models/mobilnet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e8a00227fbbeabfee3e4c7eae78ea7efab3aaaa5b33f3ff28071daefabaeab66"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

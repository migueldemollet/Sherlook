{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detection of modified images or videos using Neural Networks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageChops, ImageFilter\n",
    "import pandas as pd\n",
    "import os\n",
    "import io\n",
    "import shutil\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import backend as K\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, Activation, Input, Lambda, Resizing, GlobalAveragePooling2D\n",
    "from keras.applications import ResNet50, MobileNet, VGG16\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
    "import itertools\n",
    "\n",
    "K.clear_session()\n",
    "tf.compat.v1.reset_default_graph()\n",
    "tf.compat.v1.enable_eager_execution()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"sophatvathana/casia-dataset\"\n",
    "PATH_DATASET = './../dataset/'\n",
    "\n",
    "def download_dataset():\n",
    "    api = KaggleApi()\n",
    "    api.authenticate()\n",
    "    print(\"Downloading files...\")\n",
    "    api.dataset_download_files('sophatvathana/casia-dataset', path=PATH_DATASET, unzip=True)\n",
    "\n",
    "    print(\"\\rDownload complete.\")\n",
    "\n",
    "\n",
    "def clean_directory():\n",
    "    print(\"Moving folder...\")\n",
    "    os.rename(PATH_DATASET+\"CASIA2/Au\", PATH_DATASET+\"Au\")\n",
    "    os.rename(PATH_DATASET+\"CASIA2/Tp\", PATH_DATASET+\"Tp\")\n",
    "    \n",
    "    print(\"Cleaning directory...\")\n",
    "    shutil.rmtree(PATH_DATASET+\"casia\")\n",
    "    shutil.rmtree(PATH_DATASET+\"CASIA1\")\n",
    "    shutil.rmtree(PATH_DATASET+\"CASIA2\")\n",
    "    os.remove(PATH_DATASET+\"Tp/Thumbs.db\")\n",
    "    os.remove(PATH_DATASET+\"Au/Thumbs.db\")\n",
    "    print(\"Cleaning complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(PATH_DATASET+\"Au\"):\n",
    "    download_dataset()\n",
    "    clean_directory()\n",
    "else:\n",
    "    print(\"Dataset already Downloaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REAL_IMAGE_PATH = '../dataset/Au'\n",
    "FAKE_IMAGE_PATH = \"../dataset/Tp\"\n",
    "IMG_SIZE = (256, 256)\n",
    "CLASS = ['Manipulated', 'Original']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cabezera = \"category\", \"image\", \"real\"\n",
    "df_au = pd.DataFrame(columns=cabezera)\n",
    "\n",
    "for idx, file in enumerate(os.listdir(REAL_IMAGE_PATH)):\n",
    "    img = cv2.imread(os.path.join(REAL_IMAGE_PATH, file))\n",
    "    img = cv2.resize(img, IMG_SIZE)\n",
    "    #img_np = np.array(img)\n",
    "    category = file.split(\"_\")\n",
    "\n",
    "    df_au = pd.concat([df_au, pd.DataFrame([[category[1], img, 1]], columns=cabezera)], ignore_index=True)\n",
    "\n",
    "df_au.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove categoriy txt because it is not in the dataset\n",
    "df_au = df_au[df_au.category != \"txt\"]\n",
    "df_au = df_au[df_au.category != \"ind\"]\n",
    "#mezclar el dataframe\n",
    "df_au = df_au.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mezclar el dataframe\n",
    "df_au = df_au.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizaremos el mismo proceso con las imagenes modificadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cabezera = \"category\", \"image\", \"region\", \"real\"\n",
    "key_list = [\"ani\", \"arc\", \"art\", \"cha\", \"nat\", \"pla\", \"sec\"]\n",
    "df_tp = pd.DataFrame(columns=cabezera)\n",
    "\n",
    "for file in os.listdir(FAKE_IMAGE_PATH):\n",
    "    #convert image to np array\n",
    "    img = cv2.imread(os.path.join(FAKE_IMAGE_PATH, file))\n",
    "    img = cv2.resize(img, IMG_SIZE)\n",
    "    #img_np = np.array(img)\n",
    "    category = file.split(\"_\")\n",
    "    category[5] = category[5][:3]\n",
    "    df_tp = pd.concat([df_tp, pd.DataFrame([[category[5], img, category[1], 0]], columns=cabezera)], ignore_index=True)\n",
    "\n",
    "df_tp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tp = df_tp[df_tp.category != \"txt\"]\n",
    "df_tp = df_tp[df_tp.category != \"ind\"]\n",
    "df_tp = df_tp.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_au, df_tp], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ela_images(original, images: list, qualities: list):\n",
    "    fig = plt.figure(figsize=(20, 20))\n",
    "    fig.add_subplot(1, 4, 1)\n",
    "    plt.title(\"Original\")\n",
    "    plt.imshow(original)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    for i, image in enumerate(images):\n",
    "        fig.add_subplot(1, 4, i+2)\n",
    "        plt.title(\"Quality: \" + str(qualities[i]))\n",
    "        plt.imshow(image)\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def ela(orig_img, quality=90):\n",
    "    _, buffer = cv2.imencode('.jpg', orig_img, [cv2.IMWRITE_JPEG_QUALITY, quality])\n",
    "    compressed_img = cv2.imdecode(np.frombuffer(buffer, np.uint8), cv2.IMREAD_COLOR)\n",
    "\n",
    "    diff = 15 * cv2.absdiff(orig_img, compressed_img)\n",
    "    return diff"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating and splitting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocesar las im√°genes\n",
    "tensors_x = [tf.convert_to_tensor(ela(image)) for image in df['image']]\n",
    "tensors_y = [tf.convert_to_tensor(label) for label in df['real']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(tensors_x)\n",
    "Y = np.array(tensors_y)\n",
    "\n",
    "# Dividir el conjunto de datos en entrenamiento validacion y prueba\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    'accuracy',\n",
    "    tf.keras.metrics.Precision(name='precision'),\n",
    "    tf.keras.metrics.Recall(name='recall'),\n",
    "    tf.keras.metrics.AUC(name='auc'),\n",
    "    tf.keras.metrics.AUC(name='prc', curve='PR')\n",
    "]\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    min_delta=0, \n",
    "    patience=5, \n",
    "    verbose=0, \n",
    "    mode='auto', \n",
    "    baseline=None, \n",
    "    restore_best_weights=False\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1)\n",
    "\n",
    "\n",
    "model_chekpoint = ModelCheckpoint(\n",
    "    filepath='./../model/checkpoints', \n",
    "    monitor='val_loss', \n",
    "    verbose=0, \n",
    "    save_best_only=True,\n",
    "    save_weights_only=True, \n",
    "    mode='auto', \n",
    "    save_freq='epoch'\n",
    ")\n",
    "\n",
    "tensor_board = TensorBoard(\n",
    "    log_dir='./../model/logs',\n",
    "    histogram_freq=0,\n",
    "    write_graph=True,\n",
    "    write_images=False,\n",
    "    update_freq='epoch',\n",
    "    profile_batch=2,\n",
    "    embeddings_freq=0,\n",
    "    embeddings_metadata=None\n",
    ")\n",
    "\n",
    "callbacks = [early_stopping, model_chekpoint, tensor_board]\n",
    "optimizer = Adam(learning_rate=0.0001)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the model fron scratch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "detect_manipulated_images_model_scratch_v1.h5 8min 31s 12 epocas bacth_size=32\n",
    "\n",
    "loss: 1.6352 - accuracy: 0.7355 - precision: 0.9418 - recall: 0.3194 - auc: 0.8527 - prc: 0.8239"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(5, 5), activation=None, input_shape=(256, 256, 3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(64, kernel_size=(5, 5), activation=None, kernel_regularizer=l2(0.01)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(128, kernel_size=(5, 5), activation=None, kernel_regularizer=l2(0.01)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation=None, kernel_regularizer=l2(0.01)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the model using transfer learning (ResNet50)\n",
    "\n",
    "detect_manipulated_images_model_resnet50_v1.h5 10m 46s 7 epocas bacth_size=16\n",
    "\n",
    "loss: 0.4613 - accuracy: 0.8307 - precision: 0.8105 - recall: 0.7201 - auc: 0.9256 - prc: 0.8760\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(256, 256, 3))\n",
    "\n",
    "model = ResNet50(weights='imagenet', include_top=False, input_tensor=inputs)\n",
    "\n",
    "x = Flatten()(model.output)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=model.inputs, outputs=predictions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the model using transfer learning (MobileNet)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "detect_manipulated_images_model_mobilenet_v1.h5 4min 37s 9 epocas bacth_size=32\n",
    "\n",
    "loss: 0.6656 - accuracy: 0.8864 - precision: 0.8028 - recall: 0.9268 - auc: 0.9350 - prc: 0.8523"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(256, 256, 3))\n",
    "model = MobileNet(weights='imagenet', include_top=False, input_shape=(256, 256, 3))\n",
    "\n",
    "x = GlobalAveragePooling2D()(model.output)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=model.inputs, outputs=predictions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "detect_manipulated_images_model_mobilenet_v2.h5 1min 47s 12 epocas bacth_size=32\n",
    "\n",
    "loss: 0.5404 - accuracy: 0.7285 - precision: 0.7123 - recall: 0.4715 - auc: 0.7986 - prc: 0.6622"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MobileNet(weights='imagenet', include_top=False, input_shape=(256, 256, 3))\n",
    "\n",
    "# Congelar todas las capas del modelo pre-entrenado para que no se modifiquen durante el entrenamiento\n",
    "for layer in model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Agregar capas adicionales en la parte superior del modelo pre-entrenado\n",
    "x = model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# Definir el modelo completo\n",
    "model = Model(inputs=model.inputs, outputs=predictions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the model using transfer learning (VGG16)\n",
    "\n",
    "detect_manipulated_images_model_vgg16_v1.h5 36m 11s 15 epocas bacth_size=32\n",
    "\n",
    "loss: 0.3823 - accuracy: 0.8662 - precision: 0.8022 - recall: 0.8571 - auc: 0.9345 - prc: 0.8679"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar la arquitectura pre-entrenada VGG-16 sin las capas completamente conectadas\n",
    "vgg16 = VGG16(weights='imagenet', include_top=False, input_shape=(256, 256, 3))\n",
    "\n",
    "# Definir una nueva capa de salida personalizada\n",
    "x = vgg16.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# Construir el modelo final que incluye VGG-16 y la nueva capa de salida\n",
    "model = Model(inputs=vgg16.inputs, outputs=predictions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=metrics)\n",
    "history = model.fit(x_train, y_train, epochs=50, batch_size=32, validation_data=(x_val, y_val), callbacks=callbacks)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(history):\n",
    "    metrics = ['accuracy', 'loss', 'prc', 'precision', 'recall']\n",
    "    fig, axes = plt.subplots(len(metrics), 1, figsize=(10, 10))\n",
    "    \n",
    "    for i, metric in enumerate(metrics):\n",
    "        axes[i].plot(history.history[metric], label='train')\n",
    "        axes[i].plot(history.history[f'val_{metric}'], label='val')\n",
    "        axes[i].set_title(metric)\n",
    "        axes[i].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(model, X, y_true):\n",
    "    y_pred = model.predict(X) > 0.5\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.imshow(cm, cmap=plt.cm.Reds)\n",
    "    plt.title('Confusion Matrix', fontsize=16)\n",
    "    plt.ylabel('True label', fontsize=14)\n",
    "    plt.xlabel('Predicted label', fontsize=14)\n",
    "    plt.xticks([0, 1], ['Manipulated', 'Original'], fontsize=12)\n",
    "    plt.yticks([0, 1], ['Manipulated', 'Original'], fontsize=12)\n",
    "    plt.colorbar()\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            plt.text(j, i, str(cm[i][j]), ha='center', va='center', fontsize=20)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(model, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./../model/detect_manipulated_images_model_resNet50.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()\n",
    "model = load_model('./../model/ela_models/detect_manipulated_images_model_mobileNet_v3.h5')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probando modelo con mapa de calor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_conv_layer(model):\n",
    "    for layer in reversed(model.layers):\n",
    "        # Comprobar si la capa es una capa convolucional\n",
    "        if isinstance(layer, Conv2D):\n",
    "            return layer\n",
    "    \n",
    "    return None\n",
    "\n",
    "def predict_with_heatmap(model, img_original):\n",
    "    # Load image and convert to RGB\n",
    "    img_original = cv2.cvtColor(img_original, cv2.COLOR_BGR2RGB)\n",
    "    img_original = cv2.resize(img_original, (256, 256))\n",
    "    img = ela(img_original)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    \n",
    "    # Get predictions and last convolutional layer output\n",
    "    preds = model.predict(img)\n",
    "    last_conv_layer = get_last_conv_layer(model)\n",
    "    last_conv_layer_model = Model(model.inputs, last_conv_layer.output)\n",
    "    last_conv_output = last_conv_layer_model.predict(img)\n",
    "    \n",
    "    # Get class activation map\n",
    "    class_idx = np.argmax(preds[0])\n",
    "    class_output = model.output[:, class_idx]\n",
    "    grads = K.gradients(class_output, last_conv_layer.output)[0]\n",
    "    pooled_grads = K.mean(grads, axis=(0, 1, 2))\n",
    "    iterate = K.function([model.inputs], [pooled_grads, last_conv_layer.output[0]])\n",
    "    pooled_grads_value, last_conv_output_value = iterate([img])\n",
    "    for i in range(last_conv_output_value.shape[-1]):\n",
    "        last_conv_output_value[:, :, i] *= pooled_grads_value[i]\n",
    "    heatmap = np.mean(last_conv_output_value, axis=-1)\n",
    "    heatmap = np.maximum(heatmap, 0)\n",
    "    heatmap /= np.max(heatmap)\n",
    "    \n",
    "    # Resize heatmap to match original image size\n",
    "    heatmap = cv2.resize(heatmap, (img.shape[2], img.shape[1]))\n",
    "    \n",
    "    # Convert heatmap to RGB format\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "    \n",
    "    # Overlay heatmap on original image\n",
    "    superimposed_img = cv2.addWeighted(img_original, 0.6, heatmap, 0.4, 0)\n",
    "    \n",
    "    return img_original, superimposed_img, class_idx\n",
    "\n",
    "def plot_heatmap(img_original, superimposed_img, class_idx):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title('Original image')\n",
    "    plt.imshow(img_original)\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title('Heatmap')\n",
    "    plt.imshow(superimposed_img)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    print('Predicted class:', class_idx)\n",
    "    print('Predicted class name:', CLASS[class_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()\n",
    "model = load_model('./../model/ela_models/detect_manipulated_images_model_scratch_v1.h5')\n",
    "path = '../dataset/Tp/Tp_D_CRD_S_O_ani10103_ani10111_10637.jpg'\n",
    "img = cv2.imread(path)\n",
    "\n",
    "original_img, heatmap_img, result = predict_with_heatmap(model, img)\n",
    "plot_heatmap(original_img, heatmap_img, result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e8a00227fbbeabfee3e4c7eae78ea7efab3aaaa5b33f3ff28071daefabaeab66"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

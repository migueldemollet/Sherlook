{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detection of modified images or videos using Neural Networks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, Activation, Input, GlobalAveragePooling2D\n",
    "from keras.applications import ResNet50, MobileNet, MobileNetV2, VGG16, Xception, EfficientNetB1, EfficientNetB2, EfficientNetB3, EfficientNetV2B1\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"sophatvathana/casia-dataset\"\n",
    "PATH_DATASET = './../dataset/'\n",
    "\n",
    "def download_dataset():\n",
    "    api = KaggleApi()\n",
    "    api.authenticate()\n",
    "    print(\"Downloading files...\")\n",
    "    api.dataset_download_files('sophatvathana/casia-dataset', path=PATH_DATASET, unzip=True)\n",
    "\n",
    "    print(\"\\rDownload complete.\")\n",
    "\n",
    "\n",
    "def clean_directory():\n",
    "    print(\"Moving folder...\")\n",
    "    os.rename(PATH_DATASET+\"CASIA2/Au\", PATH_DATASET+\"Au\")\n",
    "    os.rename(PATH_DATASET+\"CASIA2/Tp\", PATH_DATASET+\"Tp\")\n",
    "    \n",
    "    print(\"Cleaning directory...\")\n",
    "    shutil.rmtree(PATH_DATASET+\"casia\")\n",
    "    shutil.rmtree(PATH_DATASET+\"CASIA1\")\n",
    "    shutil.rmtree(PATH_DATASET+\"CASIA2\")\n",
    "    os.remove(PATH_DATASET+\"Tp/Thumbs.db\")\n",
    "    os.remove(PATH_DATASET+\"Au/Thumbs.db\")\n",
    "    print(\"Cleaning complete.\")\n",
    "\n",
    "def remove_images():\n",
    "    print(\"Removing images...\")\n",
    "    for i in range(1, 4):\n",
    "        os.remove(PATH_DATASET+\"Au/Au (\"+str(i)+\").jpg\")\n",
    "        os.remove(PATH_DATASET+\"Tp/Tp (\"+str(i)+\").jpg\")\n",
    "    print(\"Removing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(PATH_DATASET+\"Au\"):\n",
    "    download_dataset()\n",
    "    clean_directory()\n",
    "else:\n",
    "    print(\"Dataset already Downloaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REAL_IMAGE_PATH = '../dataset/Au'\n",
    "FAKE_IMAGE_PATH = \"../dataset/Tp\"\n",
    "IMG_SIZE = (256, 256)\n",
    "CLASS = [\"real\", \"fake\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cabezera_au = \"category\", \"image\", \"class\"\n",
    "df_au = pd.DataFrame(columns=cabezera_au)\n",
    "\n",
    "cabezera_tp = \"category\", \"image\", \"region\", \"class\"\n",
    "df_tp = pd.DataFrame(columns=cabezera_tp)\n",
    "\n",
    "for idx, file in enumerate(os.listdir(REAL_IMAGE_PATH)):\n",
    "    img_path = os.path.join(REAL_IMAGE_PATH, file)\n",
    "    category = file.split(\"_\")\n",
    "\n",
    "    df_au = pd.concat([df_au, pd.DataFrame([[category[1], img_path, CLASS[0]]], columns=cabezera_au)], ignore_index=True)\n",
    "\n",
    "df_au = df_au[df_au.category != \"txt\"]\n",
    "df_au = df_au[df_au.category != \"ind\"]\n",
    "df_au = df_au.groupby('category').head(600)\n",
    "\n",
    "for file in os.listdir(FAKE_IMAGE_PATH):\n",
    "    img_path = os.path.join(FAKE_IMAGE_PATH, file)\n",
    "\n",
    "    category = file.split(\"_\")\n",
    "    category[5] = category[5][:3]\n",
    "    df_tp = pd.concat([df_tp, pd.DataFrame([[category[5], img_path, category[1], CLASS[1]]], columns=cabezera_tp)], ignore_index=True)\n",
    "\n",
    "df_tp = df_tp[df_tp.category != \"txt\"]\n",
    "df_tp = df_tp[df_tp.category != \"ind\"]\n",
    "df_tp = df_tp.groupby(['category', 'region']).head(300)\n",
    "df_tp = df_tp.drop(columns=['region'])\n",
    "\n",
    "df = pd.concat([df_au, df_tp], ignore_index=True)\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Preprocessing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ela(image, quality=99):\n",
    "    _, buffer = cv2.imencode('.jpg', image, [cv2.IMWRITE_JPEG_QUALITY, quality])\n",
    "    compressed_image = cv2.imdecode(np.frombuffer(buffer, np.uint8), cv2.IMREAD_COLOR)\n",
    "\n",
    "    compressed_image = compressed_image.astype(np.float32)\n",
    "\n",
    "    diff = 15 * cv2.absdiff(image, compressed_image)\n",
    "    \n",
    "    return diff\n",
    "\n",
    "def preprocess_image(image):\n",
    "    image = ela(image)\n",
    "    return tf.convert_to_tensor(image)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating and splitting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_image,\n",
    "    validation_split=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_image,\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    dataframe=df_train,\n",
    "    directory=None,\n",
    "    x_col=\"image\",\n",
    "    y_col=\"class\",\n",
    "    subset=\"training\",\n",
    "    batch_size=batch_size,\n",
    "    seed=42,\n",
    "    shuffle=True,\n",
    "    class_mode=\"binary\",\n",
    "    target_size=IMG_SIZE\n",
    ")\n",
    "\n",
    "val_generator = train_datagen.flow_from_dataframe(\n",
    "    dataframe=df_train,\n",
    "    directory=None,\n",
    "    x_col=\"image\",\n",
    "    y_col=\"class\",\n",
    "    subset=\"validation\",\n",
    "    batch_size=batch_size,\n",
    "    seed=42,\n",
    "    shuffle=True,\n",
    "    class_mode=\"binary\",\n",
    "    target_size=IMG_SIZE\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_dataframe(\n",
    "    dataframe=df_test,\n",
    "    directory=None,\n",
    "    x_col=\"image\",\n",
    "    y_col=\"class\",\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    class_mode=\"binary\",\n",
    "    target_size=IMG_SIZE\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    tf.keras.metrics.BinaryAccuracy(name='binary_accuracy'),\n",
    "    tf.keras.metrics.Precision(name='precision'),\n",
    "    tf.keras.metrics.Recall(name='recall'),\n",
    "    tf.keras.metrics.AUC(name='auc'),\n",
    "    tf.keras.metrics.AUC(name='prc', curve='PR')\n",
    "]\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    min_delta=0, \n",
    "    patience=10, \n",
    "    verbose=0, \n",
    "    mode='auto', \n",
    "    baseline=None, \n",
    "    restore_best_weights=False\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1)\n",
    "\n",
    "\n",
    "model_chekpoint = ModelCheckpoint(\n",
    "    filepath='./../model/checkpoints', \n",
    "    monitor='val_loss', \n",
    "    verbose=0, \n",
    "    save_best_only=True,\n",
    "    save_weights_only=True, \n",
    "    mode='auto', \n",
    "    save_freq='epoch'\n",
    ")\n",
    "\n",
    "tensor_board = TensorBoard(\n",
    "    log_dir='./../model/logs',\n",
    "    histogram_freq=0,\n",
    "    write_graph=True,\n",
    "    write_images=False,\n",
    "    update_freq='epoch',\n",
    "    profile_batch=2,\n",
    "    embeddings_freq=0,\n",
    "    embeddings_metadata=None\n",
    ")\n",
    "\n",
    "callbacks = [early_stopping, model_chekpoint, tensor_board]\n",
    "optimizer = Adam(1e-3)\n",
    "loss = keras.losses.BinaryCrossentropy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the model fron scratch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "detect_manipulated_images_model_scratch_v1.h5 8min 31s 12 epocas bacth_size=32\n",
    "\n",
    "loss: 1.6352 - accuracy: 0.7355 - precision: 0.9418 - recall: 0.3194 - auc: 0.8527 - prc: 0.8239"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(5, 5), activation=None, input_shape=(256, 256, 3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(64, kernel_size=(5, 5), activation=None, kernel_regularizer=l2(0.01)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(128, kernel_size=(5, 5), activation=None, kernel_regularizer=l2(0.01)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation=None, kernel_regularizer=l2(0.01)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the model using transfer learning (ResNet50)\n",
    "\n",
    "detect_manipulated_images_model_resnet50_v1.h5 10m 46s 7 epocas bacth_size=16\n",
    "\n",
    "loss: 0.4613 - accuracy: 0.8307 - precision: 0.8105 - recall: 0.7201 - auc: 0.9256 - prc: 0.8760\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(256, 256, 3))\n",
    "\n",
    "model = ResNet50(weights='imagenet', include_top=False, input_tensor=inputs)\n",
    "\n",
    "x = Flatten()(model.output)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=model.inputs, outputs=predictions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the model using transfer learning (MobileNet)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "detect_manipulated_images_model_mobilenet_v4.h5 12min 46s 12 epocas bacth_size=32\n",
    "\n",
    "loss: 0.7375 - binary_accuracy: 0.9193 - precision: 0.9753 - recall: 0.8889 - auc: 0.9468 - prc: 0.9692"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(256, 256, 3))\n",
    "model = MobileNet(weights='imagenet', include_top=False, input_shape=(256, 256, 3))\n",
    "\n",
    "x = GlobalAveragePooling2D()(model.output)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=model.inputs, outputs=predictions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "detect_manipulated_images_model_mobilenet_v2.h5 1min 47s 12 epocas bacth_size=32\n",
    "\n",
    "loss: 0.5404 - accuracy: 0.7285 - precision: 0.7123 - recall: 0.4715 - auc: 0.7986 - prc: 0.6622"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(256, 256, 3))\n",
    "model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(256, 256, 3))\n",
    "\n",
    "x = GlobalAveragePooling2D()(model.output)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=model.inputs, outputs=predictions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the model using transfer learning (VGG16)\n",
    "\n",
    "detect_manipulated_images_model_vgg16_v1.h5 36m 11s 15 epocas bacth_size=32\n",
    "\n",
    "loss: 0.3823 - accuracy: 0.8662 - precision: 0.8022 - recall: 0.8571 - auc: 0.9345 - prc: 0.8679"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16 = VGG16(weights='imagenet', include_top=False, input_shape=(256, 256, 3))\n",
    "\n",
    "x = vgg16.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=vgg16.inputs, outputs=predictions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creatin Model Xception\n",
    "\n",
    "detect_manipulated_images_model_Xception.h5 29m 38s 12 epocas bacth_size=16\n",
    "\n",
    "loss: 0.5450 - binary_accuracy: 0.8978 - precision: 0.9046 - recall: 0.9300 - auc: 0.9331 - prc: 0.9372"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(256, 256, 3))\n",
    "model = Xception(weights='imagenet', include_top=False, input_shape=(256, 256, 3))\n",
    "\n",
    "x = GlobalAveragePooling2D()(model.output)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=model.inputs, outputs=predictions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creatin Model EfficientNetB1\n",
    "detect_manipulated_images_model_EfficientNetB1.h5 34m 20s 15 epocas bacth_size=32 139s\n",
    "\n",
    "loss: 0.2146 - binary_accuracy: 0.9302 - precision: 0.9447 - recall: 0.9232 - auc: 0.9781 - prc: 0.9827"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EfficientNetB1(weights='imagenet', include_top=False, input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3))\n",
    "\n",
    "x = GlobalAveragePooling2D()(model.output)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=model.inputs, outputs=predictions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creatin Model EfficientNetB1 v2\n",
    "\n",
    "detect_manipulated_images_model_EfficientNetB1.h5 27m 59s 12 epocas bacth_size=32 140s\n",
    "\n",
    "loss: 0.2324 - binary_accuracy: 0.9325 - precision: 0.9493 - recall: 0.9383 - auc: 0.9753 - prc: 0.9816"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EfficientNetB1(weights='imagenet', include_top=False, input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3))\n",
    "\n",
    "x = GlobalAveragePooling2D()(model.output)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=model.inputs, outputs=predictions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creatin Model EfficientNetV2B1\n",
    "\n",
    "detect_manipulated_images_model_EfficientNetV2B1.h5 16m 54s 12 epocas bacth_size=32 86s\n",
    "\n",
    "loss: 0.2860 - binary_accuracy: 0.9053 - precision: 0.9108 - recall: 0.9346 - auc: 0.9719 - prc: 0.9789"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EfficientNetV2B1(weights='imagenet', include_top=False, input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3))\n",
    "\n",
    "x = GlobalAveragePooling2D()(model.output)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=model.inputs, outputs=predictions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creatin Model EfficientNetB2\n",
    "\n",
    "detect_manipulated_images_model_EfficientNetB2.h5 39m 54s 17 epocas bacth_size=32 140s\n",
    "\n",
    "loss: 0.2816 - binary_accuracy: 0.9246 - precision: 0.9118 - recall: 0.9688 - auc: 0.9739 - prc: 0.9745"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EfficientNetB2(weights='imagenet', include_top=False, input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3))\n",
    "\n",
    "x = GlobalAveragePooling2D()(model.output)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=model.inputs, outputs=predictions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creatin Model EfficientNetB3\n",
    "\n",
    "detect_manipulated_images_model_EfficientNetB3.h5 36m 56s 12 epocas bacth_size=32 180s\n",
    "\n",
    "loss: 0.3036 - binary_accuracy: 0.9061 - precision: 0.9569 - recall: 0.8920 - auc: 0.9676 - prc: 0.9838\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EfficientNetB3(weights='imagenet', include_top=False, input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3))\n",
    "\n",
    "x = GlobalAveragePooling2D()(model.output)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=model.inputs, outputs=predictions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "history = model.fit(train_generator, epochs=50, batch_size=batch_size, validation_data=val_generator, callbacks=callbacks)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_generator)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(history):\n",
    "    metrics = ['binary_accuracy', 'loss', 'prc', 'precision', 'recall']\n",
    "    fig, axes = plt.subplots(len(metrics), 1, figsize=(10, 10))\n",
    "    \n",
    "    for i, metric in enumerate(metrics):\n",
    "        axes[i].plot(history.history[metric], label='train')\n",
    "        axes[i].plot(history.history[f'val_{metric}'], label='val')\n",
    "        axes[i].set_title(metric)\n",
    "        axes[i].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(model, X, y_true):\n",
    "    y_pred = model.predict(X) > 0.5\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.imshow(cm, cmap=plt.cm.Reds)\n",
    "    plt.title('Confusion Matrix', fontsize=16)\n",
    "    plt.ylabel('True label', fontsize=14)\n",
    "    plt.xlabel('Predicted label', fontsize=14)\n",
    "    plt.xticks([0, 1], ['Manipulated', 'Original'], fontsize=12)\n",
    "    plt.yticks([0, 1], ['Manipulated', 'Original'], fontsize=12)\n",
    "    plt.colorbar()\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            plt.text(j, i, str(cm[i][j]), ha='center', va='center', fontsize=20)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(model, test_generator):\n",
    "    y_true = test_generator.labels\n",
    "    y_pred = model.predict(test_generator) >= 0.5\n",
    "   \n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(cm, cmap=plt.cm.Reds)\n",
    "    plt.title('Confusion Matrix', fontsize=16)\n",
    "    plt.ylabel('True label', fontsize=14)\n",
    "    plt.xlabel('Predicted label', fontsize=14)\n",
    "    plt.xticks([0, 1], ['Original', 'Manipulated'], fontsize=12)\n",
    "    plt.yticks([0, 1], ['Original', 'Manipulated'], fontsize=12)\n",
    "    plt.colorbar()\n",
    "    \n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            plt.text(j, i, str(cm[i][j]), ha='center', va='center', fontsize=20)\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(model, test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./../model/ela_models/detect_manipulated_images_model_EfficientNetB1.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e8a00227fbbeabfee3e4c7eae78ea7efab3aaaa5b33f3ff28071daefabaeab66"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
